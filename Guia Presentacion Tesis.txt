========================== GUIÓN ==============================

1 - [Inicio] Buenas tardes, mi nombre es Cristian Andrade, llevo dos meses trabajando como Analista de Ciencia de Datos en Olimpia y hoy quiero presentarles mi proyecto de grado con el cual culmino la carrera de Ingeniería Electrónica.

2 - [Introducción] Gran parte de los trámites que realizamos como ciudadanos requieren que validemos nuestra identidad mediante la captura de la huella dactilar. Muchas veces la imagen obtenida resulta ilegible debido a que la huella está modificada por una quemadura o una enfermedad de la piel o cierto factor hizo que la imagen resultara distorsionada o incompleta. Esto hace que el sistema no identifique correctamente a las personas lo que dificulta la realización del proceso.

3 - [Red neuronal convolucional] Para solucionar este problema decidí implementar una red neural convolucional que reconstruye y restaura huellas dactilares digitales que se encuentran deterioradas.

4 - [Modelos generativos adversariales] Estos modelos están se utilizando por ejemplo para aumentar la resolución de las imágenes con lo que se puede mejorar la información que estas contienen. Los dos rostros de la derecha corresponden a dos personas que no existen, es decir fueron construidos por un modelo generativo adversarial.

4 - [Imagen digital] Para comenzar quiero que recordemos qué es una imagen digital. Una imagen digital es un conjunto de píxeles agrupados uno al lado de otro formando una estructura que generalmente es rectangular. En una impresión dactilar cada píxel es un valor numérico que indica la cantidad de color blanco o negro hay en la imagen.

5 - [Convolución] El modelo procesa las huellas a través de la convolución. La convolución es una operación matemática en la que se utiliza un filtro para modificar la imagen. En dicha operación se multiplican y se suman los parámetros del filtro y los pixeles de una región de imagen original. Este proceso se repite por toda la imagen para obtener cada uno de los pixeles de la imagen procesada. 

5 - [Convolución] La convolución es muy conveniente porque permite extraer las crestas y los valles que forman los patrones de la huella. Esta información se procesa también mediante convoluciones lo que finalmente resulta en la imagen reconstruida y mejorada.

6 - [Modelo de reconstrucción] Concretamente el modelo consiste en un conjunto de filtros que aplican la operación de la convolución consecutivamente a la imagen de entrada hasta que se obtiene la huella reconstruida. El objetivo principal es encontrar el valor óptimo de los parámetros de cada filtro de tal forma que se obtenga el procesamiento deseado.

6 - [Arquitectura] En la figura de la izquierda podemos ver el módulo que reconstruye las huellas al cual llamaremos de ahora en adelante como generador. A la derecha observamos un segundo módulo convolucional que se denomina discriminador cuyo objetivo es decidir si una huella está correctamente reconstruida o no. La idea es formular un problema de optimización en el que se minimizan dos funciones de costo que en conjunto expresan una competencia entre el generador y el discriminador, de ahí el término adversarial.

7 - [Funciones Costo]  En la primera ecuación, el generador busca engañar al discriminador indicándole que el resultado del procesamiento de la imagen deteriorada es efectivamente una huella correctamente reconstruida. Por el contrario, en la segunda ecuación, el discriminador indica que la huella procesada por el generador no está correctamente reconstruida y que las huellas que sí los están, solamente corresponden a las imágenes originales. Al minimizar estas dos funciones de costo al mismo tiempo durante un número de iteraciones determinado, tanto el generador como el discriminador mejoran su desempeño, uno a costa de la mejora del otro.

8 - [Flujo de entrenamiento del modelo] En el siguiente diagrama se resume el proceso de entrenamiento del modelo. Primero se deteriora artificialmente la huella original, esta huella deteriorada se reconstruye con el generador y la huella obtenida se evalúa a través del discriminador. Tanto la huella original, como la huella reconstruida y el resultado del discriminador se utilizan para calcular el valor de la función de costo y luego se procede a optimizar los filtros del generador con base en el resultado obtenido. Ahora continuamos con la parte del discriminador. Tanto la huella original como la huella reconstruida se pasan por el discriminador para calcular el valor de la función de costo lo que permitirá actualizar los parámetros del discriminador. Finalmente, el proceso completo se repite hasta obtener el resultado esperado en las reconstrucciones.

9 - [Datos de entrenamiento, software y hardware] Olimpia proporcionó 100.000 huellas, de estas se escogieron aleatoriamente 18.000 para entrenamiento y 20000 para validación. Para el desarrollo del modelo se utilizó Python, el framework de machine learning Tensorflow y el cluster de cómputo de alto desempeño de la Universidad de los Andes.El tiempo de entrenamiento fue de 1 día, 27 horas y 32 minutos.

10 - [Datos de entrenamiento] El deterioro de las imágenes corresponden a dos tipos de modificación definidos: uno causado por un factor externo y otro simulando un deterioro directamente en la piel.

11 - [Curvas ROC y CMC]

Una vez el modelo es entrenado mediante el modelo descrito anteriormente, se procede a evaluar el desempeño del mismo.

La curva ROC presenta la relación que hay entre los verdaderos positivos y los falsos positivos de un sistema de identificación. Tras la reconstrucción de las huellas podemos observar que la curva se ubicó más hacia la esquina superior izquierda lo que quiere decir que aumentó el número de casos en los que el sistema aceptó una persona y efectivamente esa era la persona mientras que disminuyó el número de casos en los que el sistema aceptó a una persona y la realidad era que esa no era la persona correcta.

La métrica Cumulative Match Curve nos muestra la precisión de un sistema que identifica a una persona dentro de un grupo de n personas con base en el puntaje obtenido al medir la similaridad entre cada posible par de huellas. Se puede observar que después de la reconstrucción la precisión del modelo mejora y precisamente dentro de los k puntajes más altos.

12 - [Calidad de las huellas]

La tercera métrica consistió en medir la calidad promedio de las huella antes y después de la reconstrucción mediante un algoritmo ampliamente utilizado en el campo de la biometría que se denomina NFIQ. Está métrica califica una huella de alta calidad con un valor de 1 y una huella de baja calidad con un valor de 5. Podemos observar que después de la reconstrucción la calidad promedio de las huellas mejoró en un 43% ya que la medida pasó de 3.9 a 2.72.

13 - [Huellas restauradas]

Ahora bien, no podemos terminar sin ver cómo queda una huella reconstruida. Aquí vemos que el modelo pudo restablecer las regiones deterioradas dibujando las crestas que habían desaparecido. Adicionalmente, encontramos que el ruido de las imágenes fue eliminado lo que mejoró la claridad y la calidad de las huellas.

14 - [Test de Carga Computacional]

Para analizar la carga computacional que requiere la reconstrucción de las huellas se midió el tiempo y la memoria RAM requeridos para procesar 300 huellas. Primero se utilizó la Raspberry PI en un micro computador que se utiliza para implementar prototipos tecnológicos portables. En este la reconstrucción de la huella consume aproximadamente 593 MB de RAM y el tiempo promedio por huella es de 1.4 segundos. Por otro lado se utilizó un computador de escritorio para simular un servidor en el que se implementa una API y en este la RAM consumida fue de 612 MB y el tiempo de reconstrucción promedio por huella fue de 0.2 segundos.

15 - [Arquitectura bayesiana]

Como alternativa a la arquitectura se propuso añadir la divergencia KL a la función de costo del generador. Para calcular este término se obtiene un vector a partir de una de las convoluciones del generador. La primera y segunda mitad de los valores del vector corresponden a la media y la varianza respectivamente de la distribución de probabilidad que configura el generador al procesar las huellas.

Al minimizar esta función se busca que el generador se ajuste a la distribución de probabilidad de los huellas, lo que puede resultar en una mejor reconstrucción. Adicionalmente, disminuye el overfiting del sistema, lo que quiere decir que se evita que el modelo solo reconstruya correctamente las huellas de entrenamiento y no las de la implementación real.

En las pruebas realizadas se obtuvo un desempeño similar al del modelo descrito anteriormente.

16 - [Trabajo futuro]

Se podrían desarrollar algoritmos más sofisticados para el deterioro de las huellas de tal manera que se cuente con datos de entrenamiento más reales.

Adicionalmente, se podría utilizar la binarización de las huellas que proporciona el software biométrico NEUROTEC para utilizarla como la imagen objetivo de la reconstrucción. Esto permitiría que el modelo identificara de una manera más clara las crestas y los valles que debe restaurar.

=========== OPCIONAL ====================
Ahora bien, el modelo modifica la imagen de la huella dactilar a través de convoluciones. La convolución es una operación matemática en la que se toma un conjunto de valores numéricos distribuidos en una cuadrícula y se multiplican por los pixeles correspondientes de la imagen original. Los resultados de las multiplicaciones se suman para obtener un pixel de la imagen procesada. El proceso se repite por toda la imagen original para obtener todos los pixeles de la imagen resultante. A esta cuadrícula de valores que realiza el procesamiento se le denomina filtro.